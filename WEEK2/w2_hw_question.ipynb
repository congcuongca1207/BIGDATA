{"cells":[{"cell_type":"markdown","metadata":{"id":"YSD4QWIRqNiW"},"source":["# Comprehensive Student Data Analysis with PySpark"]},{"cell_type":"markdown","metadata":{"id":"_WJDPO1WqQeJ"},"source":["## Objective"]},{"cell_type":"markdown","metadata":{"id":"WBTofZ06qS4d"},"source":["Leverage PySpark to perform a thorough analysis of student performance data. This exercise covers data loading and manipulation using RDDs and DataFrames, and it culminates in building and evaluating a logistic regression model to predict student success.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fMJPgi9-qT4x"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"2Cw1gpIoqWWJ"},"source":["**student_data.csv** includes:\n","\n","- age: Age of the student\n","- study_time: Weekly study hours\n","- failures: Number of past class failures\n","- passed: Course outcome (1: passed, 0: failed)"]},{"cell_type":"markdown","metadata":{"id":"OgMXfD6Vq5Kc"},"source":["## Set Up"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Z2M9-B0Wq-WP"},"outputs":[],"source":["# !pip install pyspark"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"RY0Ah382rAgy"},"outputs":[],"source":["import pyspark"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"PgeLaNopq6Z3"},"outputs":[{"name":"stderr","output_type":"stream","text":["24/04/08 08:59:30 WARN Utils: Your hostname, codespaces-f38966 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n","24/04/08 08:59:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n","Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","24/04/08 08:59:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n","from pyspark.ml import Pipeline\n","\n","spark = SparkSession.builder.appName(\"Student Data Analysis\").getOrCreate()"]},{"cell_type":"markdown","metadata":{"id":"Zojffl7Yqa1s"},"source":["## Tasks"]},{"cell_type":"markdown","metadata":{"id":"aAwqdwOXqcSK"},"source":["### Task 1: Resilient Distributed Dataset (RDD) Operations"]},{"cell_type":"markdown","metadata":{"id":"01q_KVuQqfG6"},"source":["1. Load student_data.csv into an RDD and remove the header."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["24/04/08 08:59:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"]}],"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder \\\n","    .appName(\"Read CSV and Remove Header\") \\\n","    .getOrCreate()\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["student_data_rdd = spark.sparkContext.textFile(\"student_data.csv\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["header = student_data_rdd.first()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["data_without_header = student_data_rdd.filter(lambda row: row != header)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["['22,8,0,1',\n"," '19,7,2,0',\n"," '23,8,1,1',\n"," '20,6,2,0',\n"," '22,9,0,1',\n"," '18,5,3,0',\n"," '22,3,3,0',\n"," '23,3,1,0',\n"," '20,1,0,0',\n"," '19,8,1,1']"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["data_without_header.take(10)"]},{"cell_type":"markdown","metadata":{"id":"NwX_vDqsqh03"},"source":["2. Filter to include only students older than 20 years."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":435,"status":"ok","timestamp":1712160262244,"user":{"displayName":"Tien Dinh","userId":"09967600448114579438"},"user_tz":-420},"id":"YOSZHiosrfT4","outputId":"d03d84df-91a8-424c-f0d8-a506110c249e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[]\n"]}],"source":["# YOUR CODE HERE\n","students_over_20 = data_without_header.filter(lambda row: int(row.split(',')[1]) > 20)\n","\n","# View a sample of the filtered data (optional)\n","data_sample = students_over_20.take(10)\n","print(data_sample)"]},{"cell_type":"markdown","metadata":{"id":"Tvd2yv54qi3B"},"source":["3. Count students older than 20 with past failures.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":853,"status":"ok","timestamp":1712160263096,"user":{"displayName":"Tien Dinh","userId":"09967600448114579438"},"user_tz":-420},"id":"tSe2gkaprjEV","outputId":"21eeca94-0450-4982-b144-384c5a97f4aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of students older than 20 with past failures: 0\n"]}],"source":["# YOUR CODE HERE\n","failing_students_over_20 = data_without_header.filter(lambda row: \n","                                                    int(row.split(',')[1]) > 20 and \n","                                                    row.split(',')[3] == \"True\")  # Adjust index for \"HasFailed\"\n","\n","# Count the failing students\n","student_count = failing_students_over_20.count()\n","\n","# Print the count\n","print(f\"Number of students older than 20 with past failures: {student_count}\")"]},{"cell_type":"markdown","metadata":{"id":"HwBhtf0ZqmmH"},"source":["### Task 2: DataFrame Operations"]},{"cell_type":"markdown","metadata":{"id":"I_10Z42dqoqa"},"source":["1. Load student_data.csv into a DataFrame."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"uYdugcGPrwMN"},"outputs":[{"name":"stderr","output_type":"stream","text":["24/04/08 08:59:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"]},{"name":"stdout","output_type":"stream","text":["+---+----------+--------+------+\n","|age|study_time|failures|passed|\n","+---+----------+--------+------+\n","| 22|         8|       0|     1|\n","| 19|         7|       2|     0|\n","| 23|         8|       1|     1|\n","| 20|         6|       2|     0|\n","| 22|         9|       0|     1|\n","| 18|         5|       3|     0|\n","| 22|         3|       3|     0|\n","| 23|         3|       1|     0|\n","| 20|         1|       0|     0|\n","| 19|         8|       1|     1|\n","| 23|         2|       0|     0|\n","| 23|         3|       2|     0|\n","| 18|         8|       1|     1|\n","| 21|         8|       3|     0|\n","| 20|         4|       2|     0|\n","| 17|         4|       2|     0|\n","| 23|         5|       1|     1|\n","| 21|         6|       3|     0|\n","| 17|         5|       2|     0|\n","| 20|         7|       0|     1|\n","+---+----------+--------+------+\n","only showing top 20 rows\n","\n"]}],"source":["# YOUR CODE HERE\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder \\\n","    .appName(\"Load CSV to DataFrame\") \\\n","    .getOrCreate()\n","\n","# Read the CSV file into a DataFrame with inference of schema\n","df = spark.read.option(\"header\", \"true\").csv(\"student_data.csv\")\n","df.show()"]},{"cell_type":"markdown","metadata":{"id":"IrNU7Mffqphn"},"source":["2. Explore the data by displaying the schema and the first five rows."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":376,"status":"ok","timestamp":1712160264286,"user":{"displayName":"Tien Dinh","userId":"09967600448114579438"},"user_tz":-420},"id":"2FjXBAgzr0OT","outputId":"ec1f6427-3ead-4d5d-9f95-125cef4e9db5"},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- age: string (nullable = true)\n"," |-- study_time: string (nullable = true)\n"," |-- failures: string (nullable = true)\n"," |-- passed: string (nullable = true)\n","\n"]}],"source":["# YOUR CODE HERE\n","df.printSchema()"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----------+--------+------+\n","|age|study_time|failures|passed|\n","+---+----------+--------+------+\n","| 22|         8|       0|     1|\n","| 19|         7|       2|     0|\n","| 23|         8|       1|     1|\n","| 20|         6|       2|     0|\n","| 22|         9|       0|     1|\n","+---+----------+--------+------+\n","only showing top 5 rows\n","\n"]}],"source":["df.show(5)"]},{"cell_type":"markdown","metadata":{"id":"UPgNK3GhqsWn"},"source":["3. Add a new column study_time_hours converting study time from hours to minutes."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"KkNO3_cfr2qC"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----------+--------+------+----------------+\n","|age|study_time|failures|passed|study_time_hours|\n","+---+----------+--------+------+----------------+\n","| 22|         8|       0|     1|           480.0|\n","| 19|         7|       2|     0|           420.0|\n","| 23|         8|       1|     1|           480.0|\n","| 20|         6|       2|     0|           360.0|\n","| 22|         9|       0|     1|           540.0|\n","| 18|         5|       3|     0|           300.0|\n","| 22|         3|       3|     0|           180.0|\n","| 23|         3|       1|     0|           180.0|\n","| 20|         1|       0|     0|            60.0|\n","| 19|         8|       1|     1|           480.0|\n","| 23|         2|       0|     0|           120.0|\n","| 23|         3|       2|     0|           180.0|\n","| 18|         8|       1|     1|           480.0|\n","| 21|         8|       3|     0|           480.0|\n","| 20|         4|       2|     0|           240.0|\n","| 17|         4|       2|     0|           240.0|\n","| 23|         5|       1|     1|           300.0|\n","| 21|         6|       3|     0|           360.0|\n","| 17|         5|       2|     0|           300.0|\n","| 20|         7|       0|     1|           420.0|\n","+---+----------+--------+------+----------------+\n","only showing top 20 rows\n","\n"]}],"source":["# YOUR CODE HERE\n","df = df.withColumn(\"study_time_hours\", col(\"study_time\") * 60)\n","df.show()"]},{"cell_type":"markdown","metadata":{"id":"HwCI19seqt51"},"source":["4. Calculate the average age of students grouped by their pass/fail status."]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, avg"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1211,"status":"ok","timestamp":1712160265496,"user":{"displayName":"Tien Dinh","userId":"09967600448114579438"},"user_tz":-420},"id":"Mdt52dTrr4XJ","outputId":"5a579504-b522-482c-dd02-74102a14800c"},"outputs":[{"ename":"AnalysisException","evalue":"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pass/fail` cannot be resolved. Did you mean one of the following? [`passed`, `age`, `failures`, `study_time`, `study_time_hours`].;\n'Aggregate ['pass/fail], ['pass/fail, avg(cast(age#17 as double)) AS average_age#105]\n+- Project [age#17, study_time#18, failures#19, passed#20, (cast(study_time#18 as double) * cast(60 as double)) AS study_time_hours#68]\n   +- Relation [age#17,study_time#18,failures#19,passed#20] csv\n","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m average_age_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpass/fail\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maverage_age\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/sql/group.py:186\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m cast(Tuple[Column, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], exprs)\n\u001b[0;32m--> 186\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `pass/fail` cannot be resolved. Did you mean one of the following? [`passed`, `age`, `failures`, `study_time`, `study_time_hours`].;\n'Aggregate ['pass/fail], ['pass/fail, avg(cast(age#17 as double)) AS average_age#105]\n+- Project [age#17, study_time#18, failures#19, passed#20, (cast(study_time#18 as double) * cast(60 as double)) AS study_time_hours#68]\n   +- Relation [age#17,study_time#18,failures#19,passed#20] csv\n"]},{"name":"stderr","output_type":"stream","text":["24/04/08 08:59:48 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"]}],"source":["# YOUR CODE HERE\n","average_age_df = df.groupBy(\"pass/fail\").agg(avg(df[\"age\"]).alias(\"average_age\"))"]},{"cell_type":"markdown","metadata":{"id":"U-icLGf3qu30"},"source":["### Task 3: Logistic Regression Model"]},{"cell_type":"markdown","metadata":{"id":"3fLJRW_9qyUJ"},"source":["1. Prepare the data by vectorizing features and splitting into training and test datasets.\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"wITH4ocasVEv"},"outputs":[{"name":"stderr","output_type":"stream","text":["24/04/08 09:01:33 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"]}],"source":["# YOUR CODE HERE\n","from pyspark.sql import SparkSession\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.tuning import TrainValidationSplit\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","# Khởi tạo phiên Spark\n","spark = SparkSession.builder \\\n","    .appName(\"Data Preparation\") \\\n","    .getOrCreate()"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[],"source":["data_model = spark.read.csv(\"student_data.csv\", header=True, inferSchema=True)"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- age: integer (nullable = true)\n"," |-- study_time: integer (nullable = true)\n"," |-- failures: integer (nullable = true)\n"," |-- passed: integer (nullable = true)\n","\n"]}],"source":["data_model.printSchema()"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["# vector hóa các cột dữ liệu cần thiết\n","feature_cols = data_model.columns[:-1]\n","assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n","data_model = assembler.transform(data_model)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----------+--------+------+--------------+\n","|age|study_time|failures|passed|      features|\n","+---+----------+--------+------+--------------+\n","| 22|         8|       0|     1|[22.0,8.0,0.0]|\n","| 19|         7|       2|     0|[19.0,7.0,2.0]|\n","| 23|         8|       1|     1|[23.0,8.0,1.0]|\n","| 20|         6|       2|     0|[20.0,6.0,2.0]|\n","| 22|         9|       0|     1|[22.0,9.0,0.0]|\n","+---+----------+--------+------+--------------+\n","only showing top 5 rows\n","\n"]}],"source":["data_model.show(5)"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["#chia dữ liệu ra thành tệp train và tệp test\n","(train_data, test_data) = data_model.randomSplit([0.8, 0.2], seed=42)"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Số lượng dữ liệu huấn luyện: 838\n","Số lượng dữ liệu kiểm tra: 162\n"]}],"source":["print(\"Số lượng dữ liệu huấn luyện:\", train_data.count())\n","print(\"Số lượng dữ liệu kiểm tra:\", test_data.count())"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+----------+--------+------+--------------+\n","|age|study_time|failures|passed|      features|\n","+---+----------+--------+------+--------------+\n","| 16|         1|       0|     0|[16.0,1.0,0.0]|\n","| 16|         1|       0|     0|[16.0,1.0,0.0]|\n","| 16|         1|       0|     0|[16.0,1.0,0.0]|\n","| 16|         1|       0|     0|[16.0,1.0,0.0]|\n","| 16|         1|       0|     0|[16.0,1.0,0.0]|\n","+---+----------+--------+------+--------------+\n","only showing top 5 rows\n","\n"]}],"source":["train_data.show(5)"]},{"cell_type":"markdown","metadata":{"id":"F9G8omVAqz-q"},"source":["2. Build and train a logistic regression model."]},{"cell_type":"code","execution_count":48,"metadata":{"id":"kyWaaMY5sZf1"},"outputs":[],"source":["# YOUR CODE HERE\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[{"ename":"IllegalArgumentException","evalue":"label does not exist. Available: age, study_time, failures, passed, features","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)","Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n","\u001b[0;31mIllegalArgumentException\u001b[0m: label does not exist. Available: age, study_time, failures, passed, features"]}],"source":["lr_model = lr.fit(train_data)"]},{"cell_type":"markdown","metadata":{"id":"ayGJ3QWuq1qO"},"source":["3. Evaluate the model using accuracy, precision, recall, F1 score, and the area under the ROC curve.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1207,"status":"ok","timestamp":1712160272791,"user":{"displayName":"Tien Dinh","userId":"09967600448114579438"},"user_tz":-420},"id":"_rv7Of4osbU-","outputId":"18abf47d-c8d5-424e-cb55-da2097887f24"},"outputs":[{"name":"stderr","output_type":"stream","text":["24/04/04 00:07:36 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"]}],"source":["# YOUR CODE HERE"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOa5Xr03nL0wbjgCdMfJBm9","provenance":[{"file_id":"1hAJWBiM1hu0XVSAQKErpDbZEv72qOc9m","timestamp":1712160336862}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
